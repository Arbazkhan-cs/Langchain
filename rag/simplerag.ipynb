{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. \\nThe best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \\nExperiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. \\nOn the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.\\nWe show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. ', metadata={'source': 'test.txt'})]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"test.txt\")\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n      Attention? Attention!\\n    \\nDate: June 24, 2018  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n\\n[Updated on 2018-10-28: Add Pointer Network and the link to my implementation of Transformer.]\\n[Updated on 2018-11-06: Add a link to the implementation of Transformer model.]\\n[Updated on 2018-11-18: Add Neural Turing Machines.]\\n[Updated on 2019-07-18: Correct the mistake on using the term “self-attention” when introducing the show-attention-tell paper; moved it to Self-Attention section.]\\n[Updated on 2020-04-07: A follow-up post on improved Transformer models is here.]\\nAttention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence. Take the picture of a Shiba Inu in Fig. 1 as an example.\\n\\nFig. 1. A Shiba Inu in a men’s outfit. The credit of the original photo goes to Instagram @mensweardog.\\nHuman visual attention allows us to focus on a certain region with “high resolution” (i.e. look at the pointy ear in the yellow box) while perceiving the surrounding image in “low resolution” (i.e. now how about the snowy background and the outfit?), and then adjust the focal point or do the inference accordingly. Given a small patch of an image, pixels in the rest provide clues what should be displayed there. We expect to see a pointy ear in the yellow box because we have seen a dog’s nose, another pointy ear on the right, and Shiba’s mystery eyes (stuff in the red boxes). However, the sweater and blanket at the bottom would not be as helpful as those doggy features.\\nSimilarly, we can explain the relationship between words in one sentence or close context. When we see “eating”, we expect to encounter a food word very soon. The color term describes the food, but probably not so much with “eating” directly.\\n\\nFig. 2. One word \"attends\" to other words in the same sentence differently.\\nIn a nutshell, attention in deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with (or “attends to” as you may have read in many papers) other elements and take the sum of their values weighted by the attention vector as the approximation of the target.\\nWhat’s Wrong with Seq2Seq Model?#\\nThe seq2seq model was born in the field of language modeling (Sutskever, et al. 2014). Broadly speaking, it aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, question-answer dialog generation, or even parsing sentences into grammar trees.\\nThe seq2seq model normally has an encoder-decoder architecture, composed of:\\n\\nAn encoder processes the input sequence and compresses the information into a context vector (also known as sentence embedding or “thought” vector) of a fixed length. This representation is expected to be a good summary of the meaning of the whole source sequence.\\nA decoder is initialized with the context vector to emit the transformed output. The early work only used the last state of the encoder network as the decoder initial state.\\n\\nBoth the encoder and decoder are recurrent neural networks, i.e. using LSTM or GRU units.\\n\\nFig. 3. The encoder-decoder model, translating the sentence \"she is eating a green apple\" to Chinese. The visualization of both encoder and decoder is unrolled in time.\\nA critical and apparent disadvantage of this fixed-length context vector design is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. The attention mechanism was born (Bahdanau et al., 2015) to resolve this problem.\\nBorn for Translation#\\nThe attention mechanism was born to help memorize long source sentences in neural machine translation (NMT). Rather than building a single context vector out of the encoder’s last hidden state, the secret sauce invented by attention is to create shortcuts between the context vector and the entire source input. The weights of these shortcut connections are customizable for each output element.\\nWhile the context vector has access to the entire input sequence, we don’t need to worry about forgetting. The alignment between the source and target is learned and controlled by the context vector. Essentially the context vector consumes three pieces of information:\\n\\nencoder hidden states;\\ndecoder hidden states;\\nalignment between source and target.\\n\\n\\nFig. 4. The encoder-decoder model with additive attention mechanism in Bahdanau et al., 2015.\\nDefinition#\\nNow let’s define the attention mechanism introduced in NMT in a scientific way. Say, we have a source sequence $\\\\mathbf{x}$ of length $n$ and try to output a target sequence $\\\\mathbf{y}$ of length $m$:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{x} &= [x_1, x_2, \\\\dots, x_n] \\\\\\\\\\n\\\\mathbf{y} &= [y_1, y_2, \\\\dots, y_m]\\n\\\\end{aligned}\\n$$\\n\\n(Variables in bold indicate that they are vectors; same for everything else in this post.)\\nThe encoder is a bidirectional RNN (or other recurrent network setting of your choice) with a forward hidden state $\\\\overrightarrow{\\\\boldsymbol{h}}_i$ and a backward one $\\\\overleftarrow{\\\\boldsymbol{h}}_i$. A simple concatenation of two represents the encoder state. The motivation is to include both the preceding and following words in the annotation of one word.\\n\\n$$\\n\\\\boldsymbol{h}_i = [\\\\overrightarrow{\\\\boldsymbol{h}}_i^\\\\top; \\\\overleftarrow{\\\\boldsymbol{h}}_i^\\\\top]^\\\\top, i=1,\\\\dots,n\\n$$\\n\\nThe decoder network has hidden state $\\\\boldsymbol{s}_t=f(\\\\boldsymbol{s}_{t-1}, y_{t-1}, \\\\mathbf{c}_t)$ for the output word at position t, $t=1,\\\\dots,m$, where the context vector $\\\\mathbf{c}_t$ is a sum of hidden states of the input sequence, weighted by alignment scores:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\mathbf{c}_t &= \\\\sum_{i=1}^n \\\\alpha_{t,i} \\\\boldsymbol{h}_i & \\\\small{\\\\text{; Context vector for output }y_t}\\\\\\\\\\n\\\\alpha_{t,i} &= \\\\text{align}(y_t, x_i) & \\\\small{\\\\text{; How well two words }y_t\\\\text{ and }x_i\\\\text{ are aligned.}}\\\\\\\\\\n&= \\\\frac{\\\\exp(\\\\text{score}(\\\\boldsymbol{s}_{t-1}, \\\\boldsymbol{h}_i))}{\\\\sum_{i\\'=1}^n \\\\exp(\\\\text{score}(\\\\boldsymbol{s}_{t-1}, \\\\boldsymbol{h}_{i\\'}))} & \\\\small{\\\\text{; Softmax of some predefined alignment score.}}.\\n\\\\end{aligned}\\n$$\\n\\nThe alignment model assigns a score $\\\\alpha_{t,i}$ to the pair of input at position i and output at position t, $(y_t, x_i)$, based on how well they match. The set of $\\\\{\\\\alpha_{t, i}\\\\}$ are weights defining how much of each source hidden state should be considered for each output. In Bahdanau’s paper, the alignment score $\\\\alpha$ is parametrized by a feed-forward network with a single hidden layer and this network is jointly trained with other parts of the model. The score function is therefore in the following form, given that tanh is used as the non-linear activation function:\\n\\n$$\\n\\\\text{score}(\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i) = \\\\mathbf{v}_a^\\\\top \\\\tanh(\\\\mathbf{W}_a[\\\\boldsymbol{s}_t; \\\\boldsymbol{h}_i])\\n$$\\n\\nwhere both $\\\\mathbf{v}_a$ and $\\\\mathbf{W}_a$ are weight matrices to be learned in the alignment model.\\nThe matrix of alignment scores is a nice byproduct to explicitly show the correlation between source and target words.\\n\\nFig. 5. Alignment matrix of \"L\\'accord sur l\\'Espace économique européen a été signé en août 1992\" (French) and its English translation \"The agreement on the European Economic Area was signed in August 1992\". (Image source: Fig 3 in Bahdanau et al., 2015)\\nCheck out this nice tutorial by Tensorflow team for more implementation instructions.\\nA Family of Attention Mechanisms#\\nWith the help of the attention, the dependencies between source and target sequences are not restricted by the in-between distance anymore! Given the big improvement by attention in machine translation, it soon got extended into the computer vision field (Xu et al. 2015) and people started exploring various other forms of attention mechanisms (Luong, et al., 2015; Britz et al., 2017; Vaswani, et al., 2017).\\nSummary#\\nBelow is a summary table of several popular attention mechanisms and corresponding alignment score functions:\\n\\n\\n\\nName\\nAlignment score function\\nCitation\\n\\n\\n\\n\\nContent-base attention\\n$\\\\text{score}(\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i) = \\\\text{cosine}[\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i]$\\nGraves2014\\n\\n\\nAdditive(*)\\n$\\\\text{score}(\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i) = \\\\mathbf{v}_a^\\\\top \\\\tanh(\\\\mathbf{W}_a[\\\\boldsymbol{s}_{t-1}; \\\\boldsymbol{h}_i])$\\nBahdanau2015\\n\\n\\nLocation-Base\\n$\\\\alpha_{t,i} = \\\\text{softmax}(\\\\mathbf{W}_a \\\\boldsymbol{s}_t)$Note: This simplifies the softmax alignment to only depend on the target position.\\nLuong2015\\n\\n\\nGeneral\\n$\\\\text{score}(\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i) = \\\\boldsymbol{s}_t^\\\\top\\\\mathbf{W}_a\\\\boldsymbol{h}_i$where $\\\\mathbf{W}_a$ is a trainable weight matrix in the attention layer.\\nLuong2015\\n\\n\\nDot-Product\\n$\\\\text{score}(\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i) = \\\\boldsymbol{s}_t^\\\\top\\\\boldsymbol{h}_i$\\nLuong2015\\n\\n\\nScaled Dot-Product(^)\\n$\\\\text{score}(\\\\boldsymbol{s}_t, \\\\boldsymbol{h}_i) = \\\\frac{\\\\boldsymbol{s}_t^\\\\top\\\\boldsymbol{h}_i}{\\\\sqrt{n}}$Note: very similar to the dot-product attention except for a scaling factor; where n is the dimension of the source hidden state.\\nVaswani2017\\n\\n\\n\\n(*) Referred to as “concat” in Luong, et al., 2015 and as “additive attention” in Vaswani, et al., 2017.\\n(^) It adds a scaling factor $1/\\\\sqrt{n}$, motivated by the concern when the input is large, the softmax function may have an extremely small gradient, hard for efficient learning.\\nHere are a summary of broader categories of attention mechanisms:\\n\\n\\n\\nName\\nDefinition\\nCitation\\n\\n\\n\\n\\nSelf-Attention(&)\\nRelating different positions of the same input sequence. Theoretically the self-attention can adopt any score functions above, but just replace the target sequence with the same input sequence.\\nCheng2016\\n\\n\\nGlobal/Soft\\nAttending to the entire input state space.\\nXu2015\\n\\n\\nLocal/Hard\\nAttending to the part of input state space; i.e. a patch of the input image.\\nXu2015; Luong2015\\n\\n\\n\\n(&) Also, referred to as “intra-attention” in Cheng et al., 2016 and some other papers.\\nSelf-Attention#\\nSelf-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation.\\nThe long short-term memory network paper used self-attention to do machine reading. In the example below, the self-attention mechanism enables us to learn the correlation between the current words and the previous part of the sentence.\\n\\nFig. 6. The current word is in red and the size of the blue shade indicates the activation level. (Image source: Cheng et al., 2016)\\nSoft vs Hard Attention#\\nIn the show, attend and tell paper, attention mechanism is applied to images to generate captions. The image is first encoded by a CNN to extract features. Then a LSTM decoder consumes the convolution features to produce descriptive words one by one, where the weights are learned through attention. The visualization of the attention weights clearly demonstrates which regions of the image the model is paying attention to so as to output a certain word.\\n\\nFig. 7. \"A woman is throwing a frisbee in a park.\" (Image source: Fig. 6(b) in Xu et al. 2015)\\nThis paper first proposed the distinction between “soft” vs “hard” attention, based on whether the attention has access to the entire image or only a patch:\\n\\nSoft Attention: the alignment weights are learned and placed “softly” over all patches in the source image; essentially the same type of attention as in Bahdanau et al., 2015.\\n\\nPro: the model is smooth and differentiable.\\nCon: expensive when the source input is large.\\n\\n\\nHard Attention: only selects one patch of the image to attend to at a time.\\n\\nPro: less calculation at the inference time.\\nCon: the model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train. (Luong, et al., 2015)\\n\\n\\n\\nGlobal vs Local Attention#\\nLuong, et al., 2015 proposed the “global” and “local” attention. The global attention is similar to the soft attention, while the local one is an interesting blend between hard and soft, an improvement over the hard attention to make it differentiable: the model first predicts a single aligned position for the current target word and a window centered around the source position is then used to compute a context vector.\\n\\nFig. 8. Global vs local attention (Image source: Fig 2 & 3 in Luong, et al., 2015)\\nNeural Turing Machines#\\nAlan Turing in 1936 proposed a minimalistic model of computation. It is composed of a infinitely long tape and a head to interact with the tape. The tape has countless cells on it, each filled with a symbol: 0, 1 or blank (\" “). The operation head can read symbols, edit symbols and move left/right on the tape. Theoretically a Turing machine can simulate any computer algorithm, irrespective of how complex or expensive the procedure might be. The infinite memory gives a Turing machine an edge to be mathematically limitless. However, infinite memory is not feasible in real modern computers and then we only consider Turing machine as a mathematical model of computation.\\n\\nFig. 9. How a Turing machine looks like: a tape + a head that handles the tape. (Image source: http://aturingmachine.com/)\\nNeural Turing Machine (NTM, Graves, Wayne & Danihelka, 2014) is a model architecture for coupling a neural network with external memory storage. The memory mimics the Turing machine tape and the neural network controls the operation heads to read from or write to the tape. However, the memory in NTM is finite, and thus it probably looks more like a “Neural von Neumann Machine”.\\nNTM contains two major components, a controller neural network and a memory bank.\\nController: is in charge of executing operations on the memory. It can be any type of neural network, feed-forward or recurrent.\\nMemory: stores processed information. It is a matrix of size $N \\\\times M$, containing N vector rows and each has $M$ dimensions.\\nIn one update iteration, the controller processes the input and interacts with the memory bank accordingly to generate output. The interaction is handled by a set of parallel read and write heads. Both read and write operations are “blurry” by softly attending to all the memory addresses.\\n\\nFig 10. Neural Turing Machine Architecture.\\nReading and Writing#\\nWhen reading from the memory at time t, an attention vector of size $N$, $\\\\mathbf{w}_t$ controls how much attention to assign to different memory locations (matrix rows). The read vector $\\\\mathbf{r}_t$ is a sum weighted by attention intensity:\\n\\n$$\\n\\\\mathbf{r}_t = \\\\sum_{i=1}^N w_t(i)\\\\mathbf{M}_t(i)\\\\text{, where }\\\\sum_{i=1}^N w_t(i)=1, \\\\forall i: 0 \\\\leq w_t(i) \\\\leq 1\\n$$\\n\\nwhere $w_t(i)$ is the $i$-th element in $\\\\mathbf{w}_t$ and $\\\\mathbf{M}_t(i)$ is the $i$-th row vector in the memory.\\nWhen writing into the memory at time t, as inspired by the input and forget gates in LSTM, a write head first wipes off some old content according to an erase vector $\\\\mathbf{e}_t$ and then adds new information by an add vector $\\\\mathbf{a}_t$.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\tilde{\\\\mathbf{M}}_t(i) &= \\\\mathbf{M}_{t-1}(i) [\\\\mathbf{1} - w_t(i)\\\\mathbf{e}_t] &\\\\scriptstyle{\\\\text{; erase}}\\\\\\\\\\n\\\\mathbf{M}_t(i) &= \\\\tilde{\\\\mathbf{M}}_t(i) + w_t(i) \\\\mathbf{a}_t &\\\\scriptstyle{\\\\text{; add}}\\n\\\\end{aligned}\\n$$\\n\\nAttention Mechanisms#\\nIn Neural Turing Machine, how to generate the attention distribution $\\\\mathbf{w}_t$ depends on the addressing mechanisms: NTM uses a mixture of content-based and location-based addressings.\\nContent-based addressing\\nThe content-addressing creates attention vectors based on the similarity between the key vector $\\\\mathbf{k}_t$ extracted by the controller from the input and memory rows. The content-based attention scores are computed as cosine similarity and then normalized by softmax. In addition, NTM adds a strength multiplier $\\\\beta_t$ to amplify or attenuate the focus of the distribution.\\n\\n$$\\nw_t^c(i) \\n= \\\\text{softmax}(\\\\beta_t \\\\cdot \\\\text{cosine}[\\\\mathbf{k}_t, \\\\mathbf{M}_t(i)])\\n= \\\\frac{\\\\exp(\\\\beta_t \\\\frac{\\\\mathbf{k}_t \\\\cdot \\\\mathbf{M}_t(i)}{\\\\|\\\\mathbf{k}_t\\\\| \\\\cdot \\\\|\\\\mathbf{M}_t(i)\\\\|})}{\\\\sum_{j=1}^N \\\\exp(\\\\beta_t \\\\frac{\\\\mathbf{k}_t \\\\cdot \\\\mathbf{M}_t(j)}{\\\\|\\\\mathbf{k}_t\\\\| \\\\cdot \\\\|\\\\mathbf{M}_t(j)\\\\|})}\\n$$\\n\\nInterpolation\\nThen an interpolation gate scalar $g_t$ is used to blend the newly generated content-based attention vector with the attention weights in the last time step:\\n\\n$$\\n\\\\mathbf{w}_t^g = g_t \\\\mathbf{w}_t^c + (1 - g_t) \\\\mathbf{w}_{t-1} \\n$$\\n\\nLocation-based addressing\\nThe location-based addressing sums up the values at different positions in the attention vector, weighted by a weighting distribution over allowable integer shifts. It is equivalent to a 1-d convolution with a kernel $\\\\mathbf{s}_t(.)$, a function of the position offset. There are multiple ways to define this distribution. See Fig. 11. for inspiration.\\n\\nFig. 11. Two ways to represent the shift weighting distribution $\\\\mathbf{s}\\\\_t$.\\nFinally the attention distribution is enhanced by a sharpening scalar $\\\\gamma_t \\\\geq 1$.\\n\\n$$\\n\\\\begin{aligned}\\n\\\\tilde{w}_t(i) &= \\\\sum_{j=1}^N w_t^g(j) s_t(i-j) & \\\\scriptstyle{\\\\text{; circular convolution}}\\\\\\\\\\nw_t(i) &= \\\\frac{\\\\tilde{w}_t(i)^{\\\\gamma_t}}{\\\\sum_{j=1}^N \\\\tilde{w}_t(j)^{\\\\gamma_t}} & \\\\scriptstyle{\\\\text{; sharpen}}\\n\\\\end{aligned}\\n$$\\n\\nThe complete process of generating the attention vector $\\\\mathbf{w}_t$ at time step t is illustrated in Fig. 12. All the parameters produced by the controller are unique for each head. If there are multiple read and write heads in parallel, the controller would output multiple sets.\\n\\nFig. 12. Flow diagram of the addressing mechanisms in Neural Turing Machine. (Image source: Graves, Wayne & Danihelka, 2014)\\nPointer Network#\\nIn problems like sorting or travelling salesman, both input and output are sequential data. Unfortunately, they cannot be easily solved by classic seq-2-seq or NMT models, given that the discrete categories of output elements are not determined in advance, but depends on the variable input size. The Pointer Net (Ptr-Net; Vinyals, et al. 2015) is proposed to resolve this type of problems: When the output elements correspond to positions in an input sequence. Rather than using attention to blend hidden units of an encoder into a context vector (See Fig. 8), the Pointer Net applies attention over the input elements to pick one as the output at each decoder step.\\n\\nFig. 13. The architecture of a Pointer Network model. (Image source: Vinyals, et al. 2015)\\nThe Ptr-Net outputs a sequence of integer indices, $\\\\boldsymbol{c} = (c_1, \\\\dots, c_m)$ given a sequence of input vectors $\\\\boldsymbol{x} = (x_1, \\\\dots, x_n)$ and $1 \\\\leq c_i \\\\leq n$. The model still embraces an encoder-decoder framework. The encoder and decoder hidden states are denoted as $(\\\\boldsymbol{h}_1, \\\\dots, \\\\boldsymbol{h}_n)$ and $(\\\\boldsymbol{s}_1, \\\\dots, \\\\boldsymbol{s}_m)$, respectively. Note that $\\\\mathbf{s}_i$ is the output gate after cell activation in the decoder. The Ptr-Net applies additive attention between states and then normalizes it by softmax to model the output conditional probability:\\n\\n$$\\n\\\\begin{aligned}\\ny_i &= p(c_i \\\\vert c_1, \\\\dots, c_{i-1}, \\\\boldsymbol{x}) \\\\\\\\\\n    &= \\\\text{softmax}(\\\\text{score}(\\\\boldsymbol{s}_t; \\\\boldsymbol{h}_i)) = \\\\text{softmax}(\\\\mathbf{v}_a^\\\\top \\\\tanh(\\\\mathbf{W}_a[\\\\boldsymbol{s}_t; \\\\boldsymbol{h}_i]))\\n\\\\end{aligned}\\n$$\\n\\nThe attention mechanism is simplified, as Ptr-Net does not blend the encoder states into the output with attention weights. In this way, the output only responds to the positions but not the input content.\\nTransformer#\\n“Attention is All you Need”\\n(Vaswani, et al., 2017), without a doubt, is one of the most impactful and interesting paper in 2017. It presented a lot of improvements to the soft attention and make it possible to do seq2seq modeling without recurrent network units. The proposed “transformer” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture.\\nThe secret recipe is carried in its model architecture.\\nKey, Value and Query#\\nThe major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of key-value pairs, $(\\\\mathbf{K}, \\\\mathbf{V})$, both of dimension $n$ (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a query ($\\\\mathbf{Q}$ of dimension $m$) and the next output is produced by mapping this query and the set of keys and values.\\nThe transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys:\\n\\n$$\\n\\\\text{Attention}(\\\\mathbf{Q}, \\\\mathbf{K}, \\\\mathbf{V}) = \\\\text{softmax}(\\\\frac{\\\\mathbf{Q}\\\\mathbf{K}^\\\\top}{\\\\sqrt{n}})\\\\mathbf{V}\\n$$\\n\\nMulti-Head Self-Attention#\\n\\nFig. 14. Multi-head scaled dot-product attention mechanism. (Image source: Fig 2 in Vaswani, et al., 2017)\\nRather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions. I assume the motivation is because ensembling always helps? ;) According to the paper, “multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.”\\n\\n$$\\n\\\\begin{aligned}\\n\\\\text{MultiHead}(\\\\mathbf{Q}, \\\\mathbf{K}, \\\\mathbf{V}) &= [\\\\text{head}_1; \\\\dots; \\\\text{head}_h]\\\\mathbf{W}^O \\\\\\\\\\n\\\\text{where head}_i &= \\\\text{Attention}(\\\\mathbf{Q}\\\\mathbf{W}^Q_i, \\\\mathbf{K}\\\\mathbf{W}^K_i, \\\\mathbf{V}\\\\mathbf{W}^V_i)\\n\\\\end{aligned}\\n$$\\n\\nwhere $\\\\mathbf{W}^Q_i$, $\\\\mathbf{W}^K_i$, $\\\\mathbf{W}^V_i$, and $\\\\mathbf{W}^O$ are parameter matrices to be learned.\\nEncoder#\\n\\nFig. 15. The transformer’s encoder. (Image source: Vaswani, et al., 2017)\\nThe encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.\\n\\nA stack of N=6 identical layers.\\nEach layer has a multi-head self-attention layer and a simple position-wise fully connected feed-forward network.\\nEach sub-layer adopts a residual connection and a layer normalization.\\nAll the sub-layers output data of the same dimension $d_\\\\text{model} = 512$.\\n\\nDecoder#\\n\\nFig. 16. The transformer’s decoder. (Image source: Vaswani, et al., 2017)\\nThe decoder is able to retrieval from the encoded representation.\\n\\nA stack of N = 6 identical layers\\nEach layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.\\nSimilar to the encoder, each sub-layer adopts a residual connection and a layer normalization.\\nThe first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position.\\n\\nFull Architecture#\\nFinally here is the complete view of the transformer’s architecture:\\n\\nBoth the source and target sequences first go through embedding layers to produce data of the same dimension $d_\\\\text{model} =512$.\\nTo preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.\\nA softmax and linear layer are added to the final decoder output.\\n\\n\\nFig. 17. The full model architecture of the transformer. (Image source: Fig 1 & 2 in Vaswani, et al., 2017.)\\nTry to implement the transformer model is an interesting experience, here is mine: lilianweng/transformer-tensorflow. Read the comments in the code if you are interested.\\nSNAIL#\\nThe transformer has no recurrent or convolutional structure, even with the positional encoding added to the embedding vector, the sequential order is only weakly incorporated. For problems sensitive to the positional dependency like reinforcement learning, this can be a big problem.\\nThe Simple Neural Attention Meta-Learner (SNAIL) (Mishra et al., 2017) was developed partially to resolve the problem with positioning in the transformer model by combining the self-attention mechanism in transformer with temporal convolutions. It has been demonstrated to be good at both supervised learning and reinforcement learning tasks.\\n\\nFig. 18. SNAIL model architecture (Image source: Mishra et al., 2017)\\nSNAIL was born in the field of meta-learning, which is another big topic worthy of a post by itself. But in simple words, the meta-learning model is expected to be generalizable to novel, unseen tasks in the similar distribution. Read this nice introduction if interested.\\nSelf-Attention GAN#\\nSelf-Attention GAN (SAGAN; Zhang et al., 2018) adds self-attention layers into GAN to enable both the generator and the discriminator to better model relationships between spatial regions.\\nThe classic DCGAN (Deep Convolutional GAN) represents both discriminator and generator as multi-layer convolutional networks. However, the representation capacity of the network is restrained by the filter size, as the feature of one pixel is limited to a small local region. In order to connect regions far apart, the features have to be dilute through layers of convolutional operations and the dependencies are not guaranteed to be maintained.\\nAs the (soft) self-attention in the vision context is designed to explicitly learn the relationship between one pixel and all other positions, even regions far apart, it can easily capture global dependencies. Hence GAN equipped with self-attention is expected to handle details better, hooray!\\n\\nFig. 19. Convolution operation and self-attention have access to regions of very different sizes.\\nThe SAGAN adopts the non-local neural network to apply the attention computation. The convolutional image feature maps $\\\\mathbf{x}$ is branched out into three copies, corresponding to the concepts of key, value, and query in the transformer:\\n\\nKey: $f(\\\\mathbf{x}) = \\\\mathbf{W}_f \\\\mathbf{x}$\\nQuery: $g(\\\\mathbf{x}) = \\\\mathbf{W}_g \\\\mathbf{x}$\\nValue: $h(\\\\mathbf{x}) = \\\\mathbf{W}_h \\\\mathbf{x}$\\n\\nThen we apply the dot-product attention to output the self-attention feature maps:\\n\\n$$\\n\\\\begin{aligned}\\n\\\\alpha_{i,j} &= \\\\text{softmax}(f(\\\\mathbf{x}_i)^\\\\top g(\\\\mathbf{x}_j)) \\\\\\\\\\n\\\\mathbf{o}_j &= \\\\mathbf{W}_v \\\\Big( \\\\sum_{i=1}^N \\\\alpha_{i,j} h(\\\\mathbf{x}_i) \\\\Big)\\n\\\\end{aligned}\\n$$\\n\\n\\nFig. 20. The self-attention mechanism in SAGAN. (Image source: Fig. 2 in Zhang et al., 2018)\\nNote that $\\\\alpha_{i,j}$ is one entry in the attention map, indicating how much attention the model should pay to the $i$-th position when synthesizing the $j$-th location. $\\\\mathbf{W}_f$, $\\\\mathbf{W}_g$, and $\\\\mathbf{W}_h$ are all 1x1 convolution filters. If you feel that 1x1 conv sounds like a weird concept (i.e., isn’t it just to multiply the whole feature map with one number?), watch this short tutorial by Andrew Ng. The output $\\\\mathbf{o}_j$ is a column vector of the final output $\\\\mathbf{o}= (\\\\mathbf{o}_1, \\\\mathbf{o}_2, \\\\dots, \\\\mathbf{o}_j, \\\\dots, \\\\mathbf{o}_N)$.\\nFurthermore, the output of the attention layer is multiplied by a scale parameter and added back to the original input feature map:\\n\\n$$\\n\\\\mathbf{y} = \\\\mathbf{x}_i + \\\\gamma \\\\mathbf{o}_i\\n$$\\n\\nWhile the scaling parameter $\\\\gamma$ is increased gradually from 0 during the training, the network is configured to first rely on the cues in the local regions and then gradually learn to assign more weight to the regions that are further away.\\n\\nFig. 21. 128×128 example images generated by SAGAN for different classes. (Image source: Partial Fig. 6 in Zhang et al., 2018)\\n\\nCited as:\\n@article{weng2018attention,\\n  title   = \"Attention? Attention!\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2018\",\\n  url     = \"https://lilianweng.github.io/posts/2018-06-24-attention/\"\\n}\\nReferences#\\n[1] “Attention and Memory in Deep Learning and NLP.” - Jan 3, 2016 by Denny Britz\\n[2] “Neural Machine Translation (seq2seq) Tutorial”\\n[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. “Neural machine translation by jointly learning to align and translate.” ICLR 2015.\\n[4] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. “Show, attend and tell: Neural image caption generation with visual attention.” ICML, 2015.\\n[5] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” NIPS 2014.\\n[6] Thang Luong, Hieu Pham, Christopher D. Manning. “Effective Approaches to Attention-based Neural Machine Translation.” EMNLP 2015.\\n[7] Denny Britz, Anna Goldie, Thang Luong, and Quoc Le. “Massive exploration of neural machine translation architectures.” ACL 2017.\\n[8] Ashish Vaswani, et al. “Attention is all you need.” NIPS 2017.\\n[9] Jianpeng Cheng, Li Dong, and Mirella Lapata. “Long short-term memory-networks for machine reading.” EMNLP 2016.\\n[10] Xiaolong Wang, et al. “Non-local Neural Networks.” CVPR 2018\\n[11] Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. “Self-Attention Generative Adversarial Networks.” arXiv preprint arXiv:1805.08318 (2018).\\n[12] Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. “A simple neural attentive meta-learner.” ICLR 2018.\\n[13] “WaveNet: A Generative Model for Raw Audio” - Sep 8, 2016 by DeepMind.\\n[14]  Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. “Pointer networks.” NIPS 2015.\\n[15] Alex Graves, Greg Wayne, and Ivo Danihelka. “Neural turing machines.” arXiv preprint arXiv:1410.5401 (2014).\\n', metadata={'source': 'https://lilianweng.github.io/posts/2018-06-24-attention/'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2018-06-24-attention/\",), \n",
    "                        bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                            class_=(\"post-title\", \"post-content\", \"post-header\")\n",
    "                        )))\n",
    "document = loader.load()\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Java Notes  \\nUnit - 1 \\nJava: java is a n object -oriented  programing language which is developed by James Gosling  at sun \\nmicrosystems in 1991. It was initially named  Oak.  \\nJava Buzzwords:  \\n➔ Simple  \\n➔ Platform  independence  \\n➔ Secure  \\n➔ Portable  \\n➔ Multi -threaded  \\n➔ Robust  \\n➔ Reach standard libraries  \\n➔ Object oriented programing  \\nData Type  \\njava datatype is the type of variable  to store different types of data. In java there are 8 types  od data \\ntypes, byte, short, char, int, long, float, double and Boolean which are categories in 4 types.  \\n➔ Integers (byte, short, int and long)  \\n➔ Floating Point ( float and double)  \\n➔ Character (char)  \\n➔ Boolean (true/false)  \\nVariables  \\nVariables in java are the basic unit of storage in which we can store different types of data. In simple \\nterm variables are the containers in java in which we can store different data with their  different data \\ntypes. Variables have their  own scope and lifetime by which we can define the visibility and the life of \\nthe variable.  \\n➔ Defining variables: variables are defined  by defining their  data type and variable name.  \\nSyntex : \\n <data type> <variable_name> [=value (optinal)]  \\n➔ Initialization : can be assign after the defining the variable or at the time of defining it  \\nSyntex:  \\n <data_type> <variable_name> = <value/expression>  \\n <variable_name> = <value/expression> {when variable already defined>  \\n ', metadata={'source': 'Java_Full_Notes.pdf', 'page': 0}),\n",
       " Document(page_content=' \\nScope and Lifetime of Variable  \\nScope and lifetime of variable refer to the visibility and life of the variable to access them. Or we can say \\nthat it refers to the region in which the variable can be accessed. Java supports 3 types of variables:  \\n➔ Local Variables: local variables are the types of variables which can be accessed anywhere in the \\nprogram. i.e., defined in the methods, blocks, and constructor. Their  visibility is within the block \\nwhere they are initialized.  \\n➔ Instance Variables: instance variables are the types of variables which are defined within the \\nclass but outside the methods. They can access within the class and the instance (object) of that \\nclass. Their  lifetime is till the object is not dead.  \\n➔ Class Variables (Static Variables): static variables are the type of variables  which are initialized \\nwithin the class and outside the methods,  but they can only access by there class and there \\nvalue remains same for every instance of that class and associated with the class.  \\nOperators  \\nIn java, operators are the symbols which  perform operation on operands, operands can be variable, \\nliterals or expressions. In java there are 7 types of operators,  and you can memorize them by this code \\n“CAAL BUT”:  \\n➔ Comparison Operators: It is a type of operator which is used to compare two different \\nexpressions. It consist of: <, >, <=, >= , != \\n➔ Arithmetic Operators: These are type of operators which are used to perform basic mathematics \\noperations which are: +, -, /, %, *  \\n➔ Assignment Operators: these are the types of operators which are used to assign values to the \\nvariables, these are: =, -=, +=, /=, *=, %=, ^=, &=, |=, ~=, >>=, <<=, >>>=  \\n➔ Logical Operators: These are the types of operators which are used to check the relation \\nbetween two different expressions  or used to combine two different expressions , it is also know \\nas relational operators. It consists  of &&, | | \\n➔ Bitwise Operators: These are the types of operators which are used to perform on bits instead of \\nbytes, it consists of: &, |, ^, ~, >>, <<, >>>  \\n➔ Unary Operators: These are the types of operators  which are used to perfume on single \\nvariables, it consist of two operators  only which are increment and decrement: ++, -- \\n➔ Ternary Operators: These are the types of operators  which is used to perform  single line if else \\nstatement. Syntex: <var_name> = <condition > ? <statement> [true] : < <statement> [false]  \\n \\n \\n ', metadata={'source': 'Java_Full_Notes.pdf', 'page': 1}),\n",
       " Document(page_content='Control Structures  \\nControl structures in java are the type of structure which helps the developers to control the flow of \\nprogram or execution. It consists  of condition, looping and branching statements.  \\n➔ Conditional Statements:  \\no If statements  \\no If else statements  \\no If else if else statements  \\no Switch statements  \\n➔ Looping Statements:  \\no For loop:  \\nfor ( initialization; condition; update ){ \\n// code to execute  \\n} \\no While loop:  \\nwhile(condition){  \\n// code to execute  \\n} \\no Do while loop:  \\ndo{ \\n // code to execute  \\n} while(condition)  \\no For each  loop:  \\nfor(type element: array){  \\n // code to execute  \\n} \\n➔ Branching Statements:  \\no Break  \\no Continue  \\no Return statements  \\nArrays  \\nIn java, a rrays are  the type of data structure which are used to store fixed -size sequential element of \\ncollection which have same data type. In this each element has their own unique index which helps in \\naccessing the elements. This is the continent way to store and manipulate multiple values within a single \\nvariable.  \\n➔ Array Declaration: array are declared using following syntax : \\no <data_type>[] <arr_name>;  \\no <data_type> <arr_name>[];  \\n ', metadata={'source': 'Java_Full_Notes.pdf', 'page': 2}),\n",
       " Document(page_content='➔ Array size Initialization: array size is initialized in many ways:  \\no Method 1:  \\n>> Int[]num;  \\n>> num = new int[<size>];  \\no Method 2:  \\n>> int[]num = new int[10];  \\n➔ Array values initialization: arrays values are also initialized in many ways  \\no Method 1:  \\n>> int[] num = new int[] {1, 2, 3, 4, 5};  \\n>> int[] num = {1, 2, 3, 4, 5};  \\no Method 2:  \\n>> int[] num = new int[3];  \\n>> num[0] = 1;  \\n>> num[1] = 2;  \\n>> num[2] = 3;  \\n➔ Accessing Array elements: array elements can be accessed by their  unique index  \\n>> var = num[0];  \\n>> System.out.println(var);  \\n➔ Multi -dimensional Arrays: java supports multi -dimensional arrays which are array of arrays  \\no 2-D array:  \\n>>int[] num = {  \\n {1, 2, 3},  \\n {4, 5, 6}  \\n    } \\no 3-D array:  \\n>>int[] num = {  \\n {{1, 2, 3},  \\n {5, 6, 7}},  \\n{{7, 8, 9},  \\n {10, 11, 12}}  \\n} \\nJava Architecture  \\nJava architecture is the architecture which is used to create and run java programs. It consists of 3 \\ncomponents, JDK (Java Development Kit), JVM (Java Virtual Machine), JRE (Java Runtime Environment)  \\n➔ JDK: It stands for Java Development Kit (JDK)  \\no JDK is a software development kit which is used to develop java software applications.  \\no JDK provides tools and libraries to run and develop java programs . ', metadata={'source': 'Java_Full_Notes.pdf', 'page': 3}),\n",
       " Document(page_content='o JDK consists of:  \\n▪ Javac: Java compiler, it is used to compile the java code to the java byte code.  \\n▪ Java: Java interpreter, it is used to execute the java byte code in JVM.  \\n▪ Javap: Java dissembler, it is used to examine the byte code instructions.  \\n▪ Javadoc: Documentation generator, generate the document for the java program  \\n▪ Debugger: tool for debugging the java application.  \\n➔ JVM: It stands for Java Virtual Machine (JVM)  \\no JVM is used to compile the java code to the java byte code to run the java program  \\no It is used to run the java program into any machine  \\no It consists of:  \\n▪ Memory management: it is used to mange memory or to perform garbase \\ncollection auto magically . \\n▪ Used to debug the java byte code  \\n▪ Also act as an interpreter to convert the java byte code to the machine native \\nlanguage.  \\n➔ JRE: It stands for Java Runtime Environment (JRE)  \\no JRE is the subset of JDK  \\no JRE provides those tools and libraries which are use to run the java program in end user \\nlocal machine.  \\no It consists of:  \\n▪ JRE consist of JVM to run the java program  \\n▪ JRE have tools and libraries to create a runtime environment to run java program \\nplatform independently.  \\nIn summary JDK is used to develop and compile the java code, JVM is used to provide a runtime \\nenvironment and JRE is used to run the Java program in the end used application.  By combining all of \\nthem it create the Java architecture.  \\nDifference between Java and C++  \\nBasis  Java  C++ \\nLanguage Type  Object -oriented programing \\nlanguage.  Multi -paradigm programing \\nlanguage  \\nPlatform  Can run on any platform by the \\nhelp of JVM and convert the \\ncode into bytecode.  It is platform dependent, code \\ncompile into native machine \\ncode.  \\nMultiple Inheritance  Can not perform multiple \\ninheritance directly.  Can perform multiple \\ninheritance.  \\nMulti -Threading  Supports multi -threading using \\nthread class.  No concept of multi -threading . \\nMemory Management  Memory management done \\nautomatically by garbage \\ncollector  Memory management done \\nmanually by the help of pointers.  \\nPointers  Not support pointers  Supports pointer  ', metadata={'source': 'Java_Full_Notes.pdf', 'page': 4})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Java_Full_Notes.pdf\")\n",
    "document = loader.load()\n",
    "document[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\"],\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Java Notes  \\nUnit - 1 \\nJava: java is a n object -oriented  programing language which is developed by James Gosling  at sun \\nmicrosystems in 1991. It was initially named  Oak.  \\nJava Buzzwords:  \\n➔ Simple  \\n➔ Platform  independence  \\n➔ Secure  \\n➔ Portable  \\n➔ Multi -threaded  \\n➔ Robust  \\n➔ Reach standard libraries  \\n➔ Object oriented programing  \\nData Type  \\njava datatype is the type of variable  to store different types of data. In java there are 8 types  od data \\ntypes, byte, short, char, int, long, float, double and Boolean which are categories in 4 types.  \\n➔ Integers (byte, short, int and long)  \\n➔ Floating Point ( float and double)  \\n➔ Character (char)  \\n➔ Boolean (true/false)  \\nVariables  \\nVariables in java are the basic unit of storage in which we can store different types of data. In simple \\nterm variables are the containers in java in which we can store different data with their  different data', metadata={'source': 'Java_Full_Notes.pdf', 'page': 0}),\n",
       " Document(page_content='types. Variables have their  own scope and lifetime by which we can define the visibility and the life of \\nthe variable.  \\n➔ Defining variables: variables are defined  by defining their  data type and variable name.  \\nSyntex : \\n <data type> <variable_name> [=value (optinal)]  \\n➔ Initialization : can be assign after the defining the variable or at the time of defining it  \\nSyntex:  \\n <data_type> <variable_name> = <value/expression>  \\n <variable_name> = <value/expression> {when variable already defined>', metadata={'source': 'Java_Full_Notes.pdf', 'page': 0}),\n",
       " Document(page_content='Scope and Lifetime of Variable  \\nScope and lifetime of variable refer to the visibility and life of the variable to access them. Or we can say \\nthat it refers to the region in which the variable can be accessed. Java supports 3 types of variables:  \\n➔ Local Variables: local variables are the types of variables which can be accessed anywhere in the \\nprogram. i.e., defined in the methods, blocks, and constructor. Their  visibility is within the block \\nwhere they are initialized.  \\n➔ Instance Variables: instance variables are the types of variables which are defined within the \\nclass but outside the methods. They can access within the class and the instance (object) of that \\nclass. Their  lifetime is till the object is not dead.  \\n➔ Class Variables (Static Variables): static variables are the type of variables  which are initialized \\nwithin the class and outside the methods,  but they can only access by there class and there', metadata={'source': 'Java_Full_Notes.pdf', 'page': 1}),\n",
       " Document(page_content='value remains same for every instance of that class and associated with the class.  \\nOperators  \\nIn java, operators are the symbols which  perform operation on operands, operands can be variable, \\nliterals or expressions. In java there are 7 types of operators,  and you can memorize them by this code \\n“CAAL BUT”:  \\n➔ Comparison Operators: It is a type of operator which is used to compare two different \\nexpressions. It consist of: <, >, <=, >= , != \\n➔ Arithmetic Operators: These are type of operators which are used to perform basic mathematics \\noperations which are: +, -, /, %, *  \\n➔ Assignment Operators: these are the types of operators which are used to assign values to the \\nvariables, these are: =, -=, +=, /=, *=, %=, ^=, &=, |=, ~=, >>=, <<=, >>>=  \\n➔ Logical Operators: These are the types of operators which are used to check the relation \\nbetween two different expressions  or used to combine two different expressions , it is also know', metadata={'source': 'Java_Full_Notes.pdf', 'page': 1}),\n",
       " Document(page_content='as relational operators. It consists  of &&, | | \\n➔ Bitwise Operators: These are the types of operators which are used to perform on bits instead of \\nbytes, it consists of: &, |, ^, ~, >>, <<, >>>  \\n➔ Unary Operators: These are the types of operators  which are used to perfume on single \\nvariables, it consist of two operators  only which are increment and decrement: ++, -- \\n➔ Ternary Operators: These are the types of operators  which is used to perform  single line if else \\nstatement. Syntex: <var_name> = <condition > ? <statement> [true] : < <statement> [false]', metadata={'source': 'Java_Full_Notes.pdf', 'page': 1})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = splitter.split_documents(document)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector embeding and database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "embeddings = HuggingFaceHubEmbeddings(model=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.09047622233629227,\n",
       " 0.040439605712890625,\n",
       " 0.023905642330646515,\n",
       " 0.05894797295331955,\n",
       " -0.022882355377078056]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi\"\n",
    "embeddings.embed_query(text)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "db = Chroma.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Java Notes  \\nUnit - 1 \\nJava: java is a n object -oriented  programing language which is developed by James Gosling  at sun \\nmicrosystems in 1991. It was initially named  Oak.  \\nJava Buzzwords:'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what is java and who is the author of java?\"\n",
    "res = db.similarity_search(query)\n",
    "res[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o JDK consists of:  \\n▪ Javac: Java compiler, it is used to compile the java code to the java byte code.  \\n▪ Java: Java interpreter, it is used to execute the java byte code in JVM.  \\n▪ Javap: Java dissembler, it is used to examine the byte code instructions.  \\n▪ Javadoc: Documentation generator, generate the document for the java program  \\n▪ Debugger: tool for debugging the java application.  \\n➔ JVM: It stands for Java Virtual Machine (JVM)  \\no JVM is used to compile the java code to the java byte code to run the java program  \\no It is used to run the java program into any machine  \\no It consists of:  \\n▪ Memory management: it is used to mange memory or to perform garbase \\ncollection auto magically . \\n▪ Used to debug the java byte code  \\n▪ Also act as an interpreter to convert the java byte code to the machine native \\nlanguage.  \\n➔ JRE: It stands for Java Runtime Environment (JRE)  \\no JRE is the subset of JDK'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"what is java and who is the author of java?\"\n",
    "res = db.similarity_search(query)\n",
    "res[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating prompt template and loading llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='ML stands for Machine Learning, which is a subset of Artificial Intelligence (AI) that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed.\\n\\nMachine Learning is a type of algorithm that enables a computer to learn from experience and improve its performance on a task over time. The algorithm learns by recognizing patterns in data, identifying relationships between data points, and making predictions or decisions based on that information.\\n\\nThere are three main types of Machine Learning:\\n\\n1. **Supervised Learning**: In this type of learning, the algorithm is trained on labeled data, meaning the data is accompanied by correct outputs or responses. The algorithm learns to map inputs to outputs based on the labeled data and makes predictions on new, unseen data.\\n2. **Unsupervised Learning**: In this type of learning, the algorithm is trained on unlabeled data, and it must find patterns or relationships on its own. The algorithm groups similar data points together or identifies clusters or outliers in the data.\\n3. **Reinforcement Learning**: In this type of learning, the algorithm learns by interacting with an environment and receiving feedback in the form of rewards or punishments. The algorithm adjusts its behavior to maximize the rewards and minimize the punishments.\\n\\nMachine Learning has many applications, including:\\n\\n1. **Image and speech recognition**: ML algorithms can recognize objects, faces, and speech patterns with high accuracy.\\n2. **Natural Language Processing (NLP)**: ML algorithms can understand and generate human language, enabling applications like chatbots, language translation, and sentiment analysis.\\n3. **Recommendation systems**: ML algorithms can analyze user behavior and recommend products or services based on their preferences.\\n4. **Predictive maintenance**: ML algorithms can predict when equipment or machines are likely to fail, allowing for preventive maintenance.\\n5. **Healthcare**: ML algorithms can analyze medical images, diagnose diseases, and predict patient outcomes.\\n\\nMachine Learning has many benefits, including:\\n\\n1. **Improved accuracy**: ML algorithms can perform tasks more accurately than humans, especially in areas like image and speech recognition.\\n2. **Increased efficiency**: ML algorithms can automate tasks, freeing up human resources for more complex and creative work.\\n3. **Cost savings**: ML algorithms can reduce costs by predicting and preventing errors, optimizing processes, and identifying opportunities for cost savings.\\n\\nHowever, Machine Learning also has some challenges and limitations, including:\\n\\n1. **Data quality**: ML algorithms require high-quality data to learn effectively. Poor-quality data can lead to inaccurate results.\\n2. **Bias**: ML algorithms can learn biases present in the data, leading to unfair or discriminatory outcomes.\\n3. **Explainability**: ML algorithms can be difficult to interpret and explain, making it challenging to understand why they make certain decisions.\\n\\nOverall, Machine Learning is a powerful tool with many applications and benefits, but it also requires careful consideration of the challenges and limitations involved.', response_metadata={'token_usage': {'completion_tokens': 576, 'prompt_tokens': 13, 'total_tokens': 589, 'completion_time': 0.464761624, 'prompt_time': 0.002649594, 'queue_time': None, 'total_time': 0.46741121799999996}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_c4a72fb330', 'finish_reason': 'stop', 'logprobs': None}, id='run-b8bb8fdc-d12c-40df-be99-6136ad764cb3-0', usage_metadata={'input_tokens': 13, 'output_tokens': 576, 'total_tokens': 589})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0.8)\n",
    "llm.invoke(\"What is ML\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context.\n",
    "Think step by step before providing a detailed answer.\n",
    "I will tip you $1000 if the user finds the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating stuff chain to combine llm with the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), config={'run_name': 'format_inputs'})\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based only on the provided context.\\nThink step by step before providing a detailed answer.\\nI will tip you $1000 if the user finds the answer helpful.\\n<context>\\n{context}\\n</context>\\nQuestion: {input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000022988A9D4D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000022988A9E310>, model_name='llama3-8b-8192', temperature=0.8, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), config={'run_name': 'stuff_documents_chain'})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm = llm,\n",
    "    prompt = prompt\n",
    ")\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating retriever chain to retrieve the data from the vector store and pass it to the document chain for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    db.as_retriever(),\n",
    "    document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is java and who is the author of java?',\n",
       " 'context': [Document(page_content='o JDK consists of:  \\n▪ Javac: Java compiler, it is used to compile the java code to the java byte code.  \\n▪ Java: Java interpreter, it is used to execute the java byte code in JVM.  \\n▪ Javap: Java dissembler, it is used to examine the byte code instructions.  \\n▪ Javadoc: Documentation generator, generate the document for the java program  \\n▪ Debugger: tool for debugging the java application.  \\n➔ JVM: It stands for Java Virtual Machine (JVM)  \\no JVM is used to compile the java code to the java byte code to run the java program  \\no It is used to run the java program into any machine  \\no It consists of:  \\n▪ Memory management: it is used to mange memory or to perform garbase \\ncollection auto magically . \\n▪ Used to debug the java byte code  \\n▪ Also act as an interpreter to convert the java byte code to the machine native \\nlanguage.  \\n➔ JRE: It stands for Java Runtime Environment (JRE)  \\no JRE is the subset of JDK', metadata={'source': 'Java_Full_Notes.pdf', 'page': 4}),\n",
       "  Document(page_content='components, JDK (Java Development Kit), JVM (Java Virtual Machine), JRE (Java Runtime Environment)  \\n➔ JDK: It stands for Java Development Kit (JDK)  \\no JDK is a software development kit which is used to develop java software applications.  \\no JDK provides tools and libraries to run and develop java programs .', metadata={'source': 'Java_Full_Notes.pdf', 'page': 3}),\n",
       "  Document(page_content='o JRE provides those tools and libraries which are use to run the java program in end user \\nlocal machine.  \\no It consists of:  \\n▪ JRE consist of JVM to run the java program  \\n▪ JRE have tools and libraries to create a runtime environment to run java program \\nplatform independently.  \\nIn summary JDK is used to develop and compile the java code, JVM is used to provide a runtime \\nenvironment and JRE is used to run the Java program in the end used application.  By combining all of \\nthem it create the Java architecture.  \\nDifference between Java and C++  \\nBasis  Java  C++ \\nLanguage Type  Object -oriented programing \\nlanguage.  Multi -paradigm programing \\nlanguage  \\nPlatform  Can run on any platform by the \\nhelp of JVM and convert the \\ncode into bytecode.  It is platform dependent, code \\ncompile into native machine \\ncode.  \\nMultiple Inheritance  Can not perform multiple \\ninheritance directly.  Can perform multiple \\ninheritance.  \\nMulti -Threading  Supports multi -threading using', metadata={'source': 'Java_Full_Notes.pdf', 'page': 4}),\n",
       "  Document(page_content='Java Notes  \\nUnit - 1 \\nJava: java is a n object -oriented  programing language which is developed by James Gosling  at sun \\nmicrosystems in 1991. It was initially named  Oak.  \\nJava Buzzwords:  \\n➔ Simple  \\n➔ Platform  independence  \\n➔ Secure  \\n➔ Portable  \\n➔ Multi -threaded  \\n➔ Robust  \\n➔ Reach standard libraries  \\n➔ Object oriented programing  \\nData Type  \\njava datatype is the type of variable  to store different types of data. In java there are 8 types  od data \\ntypes, byte, short, char, int, long, float, double and Boolean which are categories in 4 types.  \\n➔ Integers (byte, short, int and long)  \\n➔ Floating Point ( float and double)  \\n➔ Character (char)  \\n➔ Boolean (true/false)  \\nVariables  \\nVariables in java are the basic unit of storage in which we can store different types of data. In simple \\nterm variables are the containers in java in which we can store different data with their  different data', metadata={'source': 'Java_Full_Notes.pdf', 'page': 0})],\n",
       " 'answer': 'Based on the provided context, the answer is:\\n\\nJava is an object-oriented programming language, and it was developed by James Gosling at Sun Microsystems in 1991.'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"what is java and who is the author of java?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(question):\n",
    "    return retrieval_chain.invoke({\"input\": question})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, here is a simple code for class creation in Java:\n",
      "\n",
      "```java\n",
      "class MyClass{\n",
      "    // Properties (fields)\n",
      "    public int myInt;\n",
      "    private String myString;\n",
      "\n",
      "    // Constructors\n",
      "    public MyClass(){   // default constructor\n",
      "        myInt = 0;\n",
      "        myString = \"\";\n",
      "    }\n",
      "    public MyClass(int num, String str){   // parameterized constructor\n",
      "        myInt = num;\n",
      "        myString = str;\n",
      "    }\n",
      "    public MyClass(MyClass obj){  // copy constructor\n",
      "        myInt = obj.myInt;\n",
      "        myString = obj.myString;\n",
      "    }\n",
      "\n",
      "    // Methods\n",
      "    public void setValues(int num, String str){\n",
      "        myInt = num;\n",
      "        myString = str;\n",
      "    }\n",
      "    public void getValues(){\n",
      "        System.out.println(\"myInt = \" + myInt);\n",
      "        System.out.println(\"myStr = \"+ myString);\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Note that this code defines a class `MyClass` with properties `myInt` and `myString`, two constructors (default and parameterized), and two methods (`setValues` and `getValues`).\n"
     ]
    }
   ],
   "source": [
    "response = ask_question(\"Provide me the simple code for class creation\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
